{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a6b237-765f-4fe3-a03f-b22922a5afae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.20.1+cu121)\n",
      "Collecting validators\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting yacs>=0.1.6 (from fvcore)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting termcolor>=1.1 (from fvcore)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting tabulate (from fvcore)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting iopath>=0.1.7 (from fvcore)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (6.1.0)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (72.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Building wheels for collected packages: fvcore, iopath\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61399 sha256=12d71307138e19c902bec940c47d723e7a5c8f1e06b00e7483ad7368fe7d24ff\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=2b75a16435f6995d67f3d49b779e4c9b6e706930e0ff6c43aa0ec00bae1c2a67\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
      "Successfully built fvcore iopath\n",
      "Installing collected packages: yacs, xxhash, validators, tzdata, typing-inspection, termcolor, tabulate, smmap, setproctitle, sentry-sdk, safetensors, regex, pydantic-core, pyarrow, protobuf, propcache, portalocker, multidict, hf-xet, frozenlist, docker-pycreds, dill, annotated-types, aiohappyeyeballs, yarl, pydantic, pandas, multiprocess, iopath, huggingface_hub, gitdb, aiosignal, tokenizers, gitpython, fvcore, aiohttp, wandb, transformers, bitsandbytes, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 bitsandbytes-0.45.5 datasets-3.6.0 dill-0.3.8 docker-pycreds-0.4.0 frozenlist-1.6.0 fvcore-0.1.5.post20221221 gitdb-4.0.12 gitpython-3.1.44 hf-xet-1.1.0 huggingface_hub-0.31.1 iopath-0.1.10 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 portalocker-3.1.1 propcache-0.3.1 protobuf-6.30.2 pyarrow-20.0.0 pydantic-2.11.4 pydantic-core-2.33.2 regex-2024.11.6 safetensors-0.5.3 sentry-sdk-2.27.0 setproctitle-1.3.6 smmap-5.0.2 tabulate-0.9.0 termcolor-3.1.0 tokenizers-0.21.1 transformers-4.51.3 typing-inspection-0.4.0 tzdata-2025.2 validators-0.35.0 wandb-0.19.11 xxhash-3.5.0 yacs-0.1.8 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets huggingface_hub torch torchvision validators transformers fvcore bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a8e27d-2f5f-4224-98d3-6c3f7e494a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gigagan-pytorch==0.2.20\n",
      "  Downloading gigagan_pytorch-0.2.20-py3-none-any.whl.metadata (907 bytes)\n",
      "Collecting accelerate (from gigagan-pytorch==0.2.20)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting beartype (from gigagan-pytorch==0.2.20)\n",
      "  Downloading beartype-0.20.2-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting einops>=0.6 (from gigagan-pytorch==0.2.20)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ema-pytorch (from gigagan-pytorch==0.2.20)\n",
      "  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n",
      "Collecting kornia (from gigagan-pytorch==0.2.20)\n",
      "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numerize (from gigagan-pytorch==0.2.20)\n",
      "  Downloading numerize-0.12.tar.gz (2.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting open-clip-torch<3.0.0,>=2.0.0 (from gigagan-pytorch==0.2.20)\n",
      "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from gigagan-pytorch==0.2.20) (10.2.0)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.11/site-packages (from gigagan-pytorch==0.2.20) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from gigagan-pytorch==0.2.20) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gigagan-pytorch==0.2.20) (4.66.5)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2024.11.6)\n",
      "Collecting ftfy (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.31.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.5.3)\n",
      "Collecting timm (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20)\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->gigagan-pytorch==0.2.20) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.6->gigagan-pytorch==0.2.20) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate->gigagan-pytorch==0.2.20) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->gigagan-pytorch==0.2.20) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->gigagan-pytorch==0.2.20) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate->gigagan-pytorch==0.2.20) (6.0.2)\n",
      "Collecting kornia_rs>=0.1.9 (from kornia->gigagan-pytorch==0.2.20)\n",
      "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (1.1.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from ftfy->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.6->gigagan-pytorch==0.2.20) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2024.8.30)\n",
      "Downloading gigagan_pytorch-0.2.20-py3-none-any.whl (30 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading beartype-0.20.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n",
      "Downloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: numerize\n",
      "  Building wheel for numerize (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numerize: filename=numerize-0.12-py3-none-any.whl size=3154 sha256=2bdd6a1dcad8bf79e9b1a7c44edffb4c5c34415af849872500979403240c9bd0\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/49/3e/5375462d832133c3684a27f9ad763a61141a802aee4bd445d6\n",
      "Successfully built numerize\n",
      "Installing collected packages: numerize, kornia_rs, ftfy, einops, beartype, kornia, ema-pytorch, accelerate, timm, open-clip-torch, gigagan-pytorch\n",
      "Successfully installed accelerate-1.6.0 beartype-0.20.2 einops-0.8.1 ema-pytorch-0.7.7 ftfy-6.3.1 gigagan-pytorch-0.2.20 kornia-0.8.1 kornia_rs-0.1.9 numerize-0.12 open-clip-torch-2.32.0 timm-1.0.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!pip install gigagan-pytorch==0.2.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32c33e8-cf6d-41b8-84ef-052fddfa0aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msb9855\u001b[0m (\u001b[33msb9855-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "#Replace with actual key\n",
    "wandb.login(key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1147839a-f800-45c9-9625-eab87e79711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.cuda.amp import autocast # Automatic Mixed Precision\n",
    "import bitsandbytes as bnb # For 8-bit quantization\n",
    "import torch.profiler # For profiling\n",
    "from torchvision.utils import save_image # To save images (local saving optional)\n",
    "import traceback # For detailed error messages\n",
    "import wandb # Weights & Biases\n",
    "\n",
    "# Attempt to import GigaGAN, will be mocked if not available for basic structure\n",
    "try:\n",
    "    from gigagan_pytorch import GigaGAN, Discriminator # Assuming Discriminator might be needed for type hints or explicit construction\n",
    "except ImportError:\n",
    "    print(\"Warning: gigagan_pytorch library not found. Using a mock GigaGAN class for demonstration purposes.\")\n",
    "    # Define a mock GigaGAN if the library isn't available\n",
    "    class GigaGAN(nn.Module):\n",
    "        def __init__(self, generator, discriminator, amp=False, train_upsampler=False, **kwargs): # Added discriminator to match expected signature\n",
    "            super().__init__()\n",
    "            g_config = generator if generator else {}\n",
    "            d_config = discriminator if discriminator else {} # Use discriminator config\n",
    "            s_config = g_config.get('style_network', {})\n",
    "\n",
    "            self.G = nn.Module() \n",
    "            self.G.style_network = nn.Sequential(nn.Linear(s_config.get('dim', 64), s_config.get('dim', 64)))\n",
    "            self.G.dim = g_config.get('dim', 32)\n",
    "            self.G.image_size = g_config.get('image_size', 256)\n",
    "            self.G.input_image_size = g_config.get('input_image_size', 64)\n",
    "            self.G.unconditional = g_config.get('unconditional', True)\n",
    "            self.G.unet = nn.Module() # Mock unet\n",
    "            self.G.unet.allowable_rgb_resolutions = [self.G.input_image_size] # Mock property\n",
    "\n",
    "            self.G.conv1 = nn.Conv2d(3, self.G.dim, kernel_size=3, padding=1)\n",
    "            self.G.relu1 = nn.ReLU()\n",
    "            dummy_conv_out_size = self.G.dim * (self.G.input_image_size // 2)**2 if self.G.input_image_size > 0 and self.G.dim > 0 else 2048\n",
    "            self.G.linear_large = nn.Linear(max(1, dummy_conv_out_size), max(1, dummy_conv_out_size) * 2)\n",
    "            nn.init.normal_(self.G.linear_large.weight, mean=0, std=0.01)\n",
    "            if self.G.linear_large.bias is not None: nn.init.zeros_(self.G.linear_large.bias)\n",
    "\n",
    "            self.G.final_conv = nn.Conv2d(self.G.dim if self.G.dim > 0 else 3, 3, kernel_size=3, padding=1)\n",
    "            self.G.sigmoid = nn.Sigmoid()\n",
    "            self.G.attention_block = nn.MultiheadAttention(embed_dim=max(1,self.G.dim), num_heads=1) if self.G.dim > 0 else nn.Identity()\n",
    "            self.G.norm_layer = nn.LayerNorm(max(1,self.G.dim)) if self.G.dim > 0 else nn.Identity()\n",
    "            self.G.style_processor = nn.Linear(max(1,self.G.dim), max(1,self.G.dim)) if self.G.dim > 0 else nn.Identity()\n",
    "            self.G.mapping_network = nn.Linear(max(1,self.G.dim), max(1,self.G.dim)) if self.G.dim > 0 else nn.Identity()\n",
    "            \n",
    "            self.amp = amp # Store amp\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        def load(self, checkpoint_path):\n",
    "            print(f\"MockGigaGAN: Simulating loading weights from {checkpoint_path}\")\n",
    "            return self\n",
    "\n",
    "        def generate(self, lowres_image=None, batch_size=1, *args, **kwargs):\n",
    "            current_batch_size = lowres_image.shape[0] if lowres_image is not None else batch_size\n",
    "            final_image_size = self.G.image_size\n",
    "            if lowres_image is None: \n",
    "                lowres_image = torch.randn(current_batch_size, 3, self.G.input_image_size, self.G.input_image_size, device=self.device)\n",
    "                if self.amp and self.device.type == 'cuda': lowres_image = lowres_image.half()\n",
    "            dtype = torch.float16 if self.amp and self.device.type == 'cuda' else torch.float32\n",
    "            generated_image = torch.rand(current_batch_size, 3, final_image_size, final_image_size, device=self.device, dtype=dtype)\n",
    "            return generated_image\n",
    "\n",
    "        def to(self, *args, **kwargs):\n",
    "            super().to(*args, **kwargs)\n",
    "            try: self.device = next(self.parameters()).device\n",
    "            except StopIteration:\n",
    "                found_device = None\n",
    "                for arg in args:\n",
    "                    if isinstance(arg, (str, torch.device)): found_device = torch.device(arg); break\n",
    "                if found_device: self.device = found_device\n",
    "                elif 'device' in kwargs: self.device = torch.device(kwargs['device'])\n",
    "            if hasattr(self, 'G'): self.G.to(self.device)\n",
    "            return self\n",
    "    \n",
    "    # Define a dummy Discriminator if the real one is not available for type hints\n",
    "    if 'Discriminator' not in globals():\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, **kwargs):\n",
    "                super().__init__()\n",
    "                print(f\"Mock Discriminator initialized with {kwargs}\")\n",
    "                self.fc = nn.Linear(10,1) # Dummy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc3eb63-b3b0-4cb2-afa1-1d0744c9e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GigaGANQuantizedV100(GigaGAN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.amp = kwargs.get('amp', False) \n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.device is typically set by .to(device) call after instantiation\n",
    "        # or inherited if superclass sets it based on parameters.\n",
    "\n",
    "    def quantize_for_v100(self):\n",
    "        print(\"Starting quantization for V100 GPU...\")\n",
    "        if not hasattr(self, 'G') or self.G is None:\n",
    "            raise AttributeError(\"Generator 'G' is not initialized. Cannot quantize.\")\n",
    "        \n",
    "        if not hasattr(self, 'device') or self.device is None: # Ensure device is known\n",
    "            try: self.device = next(self.G.parameters()).device\n",
    "            except StopIteration: self.device = torch.device(\"cpu\"); print(\"Warning: G has no params, device defaulted to CPU for quantize.\")\n",
    "        print(f\"Generator 'G' target device for quantization: {self.device}\")\n",
    "\n",
    "        if not list(self.G.parameters()):\n",
    "             print(\"Warning: Generator 'G' has no parameters. Quantization steps might not be effective.\")\n",
    "        else:\n",
    "            print(f\"Generator 'G' current device: {next(self.G.parameters()).device}\")\n",
    "\n",
    "        if self.device.type == 'cuda':\n",
    "            self.G.half()\n",
    "            print(\"Model G converted to FP16.\")\n",
    "        else:\n",
    "            print(\"Skipping .half() conversion as model is not on CUDA.\")\n",
    "\n",
    "        self._preserve_critical_components(self.G)\n",
    "        print(\"Critical components preservation step complete.\")\n",
    "\n",
    "        if self.device.type == 'cuda': \n",
    "            self._selective_int8_conversion()\n",
    "            print(\"Selective INT8 quantization step complete.\")\n",
    "        else:\n",
    "            print(\"Skipping INT8 conversion as model is not on CUDA.\")\n",
    "\n",
    "        self._add_stability_hooks()\n",
    "        print(\"Stability hooks step complete.\")\n",
    "        print(\"Quantization complete!\")\n",
    "\n",
    "    def _preserve_critical_components(self, module):\n",
    "        for name, child in module.named_children():\n",
    "            if any(x in name.lower() for x in ['attention', 'norm', 'style', 'mapping']):\n",
    "                child.float()\n",
    "                for param in child.parameters(): param.data = param.data.float()\n",
    "                self._preserve_critical_components(child) \n",
    "            elif len(list(child.children())) > 0:\n",
    "                self._preserve_critical_components(child)\n",
    "\n",
    "    def _selective_int8_conversion(self):\n",
    "        total_converted = 0\n",
    "        if not hasattr(self, 'G') or not (hasattr(self, 'device') and self.device.type == 'cuda'):\n",
    "            print(\"Skipping INT8: G not found or not on CUDA device.\")\n",
    "            return\n",
    "        if not list(self.G.parameters()) or not next(self.G.parameters()).is_cuda:\n",
    "             print(\"Warning: G is not on CUDA or has no parameters. Skipping bnb INT8 conversion.\")\n",
    "             return\n",
    "\n",
    "        for name, module_instance in self.G.named_modules():\n",
    "            if (isinstance(module_instance, nn.Linear) and\n",
    "                hasattr(module_instance, 'weight') and module_instance.weight is not None and\n",
    "                module_instance.weight.numel() > 1_000_000 and # Example threshold\n",
    "                not any(x in name.lower() for x in ['attention', 'style', 'mapping', 'norm'])):\n",
    "                name_parts = name.rsplit('.', 1)\n",
    "                parent_name_str = name_parts[0] if len(name_parts) > 1 else ''\n",
    "                layer_attr_name = name_parts[1] if len(name_parts) > 1 else name\n",
    "                parent_module = self.G\n",
    "                if parent_name_str:\n",
    "                    try: parent_module = self.G.get_submodule(parent_name_str)\n",
    "                    except AttributeError: print(f\"Warn: Parent submod '{parent_name_str}' for '{name}' not found. Skip INT8.\"); continue\n",
    "                if not hasattr(parent_module, layer_attr_name): print(f\"Warn: Layer '{layer_attr_name}' for '{name}' not in parent. Skip INT8.\"); continue\n",
    "                \n",
    "                original_layer = getattr(parent_module, layer_attr_name)\n",
    "                if isinstance(original_layer, bnb.nn.Linear8bitLt): continue\n",
    "                try:\n",
    "                    new_linear = bnb.nn.Linear8bitLt(\n",
    "                        original_layer.in_features, original_layer.out_features,\n",
    "                        bias=original_layer.bias is not None, has_fp16_weights=True, threshold=6.0)\n",
    "                    fp16_weight_data = original_layer.weight.data.clone().to(self.device, dtype=torch.float16)\n",
    "                    new_linear.weight = bnb.nn.Int8Params(fp16_weight_data, requires_grad=False, has_fp16_weights=True)\n",
    "                    if original_layer.bias is not None:\n",
    "                        new_linear.bias = original_layer.bias.data.clone().to(self.device, dtype=torch.float16)\n",
    "                    setattr(parent_module, layer_attr_name, new_linear.to(self.device))\n",
    "                    total_converted += 1\n",
    "                except Exception as e: print(f\"Failed to convert {name} to INT8: {e}\")\n",
    "        if total_converted > 0: print(f\"Total layers converted to INT8: {total_converted}\")\n",
    "        else: print(\"No eligible linear layers found or converted to INT8.\")\n",
    "\n",
    "    def _add_stability_hooks(self):\n",
    "        if not hasattr(self, 'G'): return\n",
    "        def pre_forward_hook(module, input_args):\n",
    "            if hasattr(module, 'parameters') and len(list(module.parameters())) > 0 and \\\n",
    "               next(module.parameters()).is_cuda and next(module.parameters()).dtype == torch.float16:\n",
    "                if isinstance(input_args[0], torch.Tensor) and input_args[0].dtype == torch.float32:\n",
    "                     return (input_args[0].half(),) + input_args[1:] if len(input_args) > 1 else (input_args[0].half(),)\n",
    "            return input_args\n",
    "        for name, module_instance in self.G.named_modules(): \n",
    "            if isinstance(module_instance, nn.Conv2d) and len(list(module_instance.parameters())) > 0 and next(module_instance.parameters()).is_cuda:\n",
    "                module_instance.register_forward_pre_hook(pre_forward_hook)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        if not hasattr(self, 'device') or self.device is None:\n",
    "             try: self.device = next(self.G.parameters()).device if hasattr(self, 'G') and list(self.G.parameters()) else torch.device(\"cpu\")\n",
    "             except StopIteration: self.device = torch.device(\"cpu\"); print(\"Warn: Device for generate() defaulted to CPU.\")\n",
    "        \n",
    "        for key_to_check in ['noise', 'lowres_image']:\n",
    "            if key_to_check in kwargs and kwargs[key_to_check] is not None:\n",
    "                tensor_input = kwargs[key_to_check]\n",
    "                if isinstance(tensor_input, torch.Tensor):\n",
    "                    target_dtype = torch.float16 if self.amp and self.device.type == 'cuda' else tensor_input.dtype\n",
    "                    kwargs[key_to_check] = tensor_input.to(self.device, dtype=target_dtype)\n",
    "        autocast_enabled = self.amp and self.device.type == 'cuda'\n",
    "        with autocast(enabled=autocast_enabled):\n",
    "            with torch.inference_mode():\n",
    "                if 'truncation_psi' not in kwargs and hasattr(self.G, 'truncation_psi'): kwargs['truncation_psi'] = 0.7\n",
    "                generated = super().generate(*args, **kwargs)\n",
    "                if isinstance(generated, torch.Tensor): generated = generated.float()\n",
    "                return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82662eb-2ea9-4767-bc21-0eed9d7f942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quantized_gigagan(checkpoint_path, device_str='cuda', unconditional_gan=True, image_size=256, input_image_size=64):\n",
    "    current_device = torch.device(device_str)\n",
    "    print(f\"Initializing GigaGANQuantizedV100 on device: {current_device}\")\n",
    "    is_upsampler = input_image_size < image_size\n",
    "    \n",
    "    # This is a critical assumption for multiscale_input_resolutions.\n",
    "    # If the real GigaGAN has a way to report allowable resolutions (e.g., via unet), that would be more robust.\n",
    "    # For now, based on past errors, it seems tied to the generator's input_image_size.\n",
    "    current_multiscale_resolution = [input_image_size] \n",
    "    print(f\"Setting D multiscale_input_resolutions to: {current_multiscale_resolution}\")\n",
    "\n",
    "    gan = GigaGANQuantizedV100(\n",
    "        train_upsampler=True,  # Assuming upsampler model\n",
    "        generator=dict(\n",
    "            style_network=dict(\n",
    "                dim=64,\n",
    "                depth=4\n",
    "            ),\n",
    "            dim=32,\n",
    "            image_size=256,\n",
    "            input_image_size=64,\n",
    "            unconditional=True\n",
    "        ),\n",
    "        discriminator=dict(\n",
    "            dim_capacity=16,\n",
    "            dim_max=512,\n",
    "            image_size=256,\n",
    "            num_skip_layers_excite=4,\n",
    "            multiscale_input_resolutions=(128,),\n",
    "            unconditional=True\n",
    "        ),\n",
    "        amp=True\n",
    "    ).to('cuda') # Move to device\n",
    "    \n",
    "    # Ensure self.device is set on gan instance after .to()\n",
    "    #gan.device = current_device \n",
    "    if hasattr(gan.G, 'parameters') and len(list(gan.G.parameters())) > 0 :\n",
    "        print(f\"Model G device after .to(): {next(gan.G.parameters()).device}\")\n",
    "    else:\n",
    "        print(\"Model G has no parameters or not fully initialized for device check after .to()\")\n",
    "\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            gan.load(checkpoint_path) \n",
    "            print(f\"Successfully called gan.load({checkpoint_path})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: gan.load() failed for {checkpoint_path}: {e}. Using initialized weights.\")\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint {checkpoint_path} not found. Using initialized weights.\")\n",
    "\n",
    "    gan.quantize_for_v100() \n",
    "    gan.eval()\n",
    "    print(\"Model is in evaluation mode.\")\n",
    "    return gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81ba9c18-c3cd-4979-a721-4944de906fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_and_profile(gan, num_images=5, output_dir=\"generated_images_quantized_profiler\", seed=42, profile_log_dir='./profiler_logs', export_chrome_trace=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(profile_log_dir, exist_ok=True) \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if not hasattr(gan, 'device') or gan.device is None: # Should be set by load_quantized_gigagan\n",
    "        gan.device = next(gan.parameters()).device if list(gan.parameters()) else torch.device(\"cpu\")\n",
    "    print(f\"Generating images on device: {gan.device}\")\n",
    "\n",
    "    input_h = gan.G.input_image_size if hasattr(gan.G, 'input_image_size') else 64\n",
    "    input_w = gan.G.input_image_size if hasattr(gan.G, 'input_image_size') else 64\n",
    "    is_upsampler_model = hasattr(gan.G, 'input_image_size') and gan.G.input_image_size < gan.G.image_size\n",
    "\n",
    "    example_input_tensor = None; generate_kwargs = {}\n",
    "    if is_upsampler_model or not gan.G.unconditional:\n",
    "        print(f\"Model type: Upsampler/Conditional. Dummy input size: ({input_h}, {input_w}).\")\n",
    "        example_input_tensor = torch.randn(1, 3, input_h, input_w, device=gan.device)\n",
    "        generate_kwargs['lowres_image'] = example_input_tensor\n",
    "    else: \n",
    "        print(f\"Model type: Unconditional from-scratch. generate_kwargs['batch_size']=1.\")\n",
    "        generate_kwargs['batch_size'] = 1\n",
    "    if example_input_tensor is not None and gan.amp and gan.device.type == 'cuda':\n",
    "        example_input_tensor = example_input_tensor.half()\n",
    "        if 'lowres_image' in generate_kwargs: generate_kwargs['lowres_image'] = example_input_tensor\n",
    "    if example_input_tensor is not None: print(f\"Dummy input tensor: shape {example_input_tensor.shape}, dtype {example_input_tensor.dtype}\")\n",
    "    \n",
    "    profiler_schedule = torch.profiler.schedule(wait=1, warmup=1, active=max(1, num_images-2), repeat=1)\n",
    "    print(f\"\\nStarting profiling session (wait=1, warmup=1, active={max(1, num_images-2)})...\")\n",
    "    profiler_activities = [torch.profiler.ProfilerActivity.CPU]\n",
    "    if gan.device.type == 'cuda': profiler_activities.append(torch.profiler.ProfilerActivity.CUDA)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=profiler_activities, schedule=profiler_schedule, record_shapes=True,\n",
    "        profile_memory=True, with_stack=True, with_flops=True\n",
    "    ) as prof:\n",
    "        for i in range(num_images):\n",
    "            with torch.no_grad(): image = gan.generate(**generate_kwargs)\n",
    "            if i < 3 and wandb.run: # Log a few sample images to wandb\n",
    "                try:\n",
    "                    image_cpu_wandb = image.float().cpu().clamp(0.,1.)\n",
    "                    wandb.log({f\"generated_sample_{i}\": wandb.Image(image_cpu_wandb)}, step=wandb.run.step) # Use global step if available or i\n",
    "                except Exception as e_wandb_img:\n",
    "                    print(f\"Warning: Could not log image {i} to wandb: {e_wandb_img}\")\n",
    "            if prof: prof.step()\n",
    "    print(f\"\\nFinished generating {num_images} images (local saving commented out during profiling).\")\n",
    "    \n",
    "    if prof and wandb.run: # Ensure profiler ran and wandb is active\n",
    "        print(\"\\n--- PyTorch Profiler Results (logged to wandb) ---\")\n",
    "        key_averages = prof.key_averages()\n",
    "        sort_key = \"self_cpu_time_total\"; has_cuda_events = False\n",
    "        if gan.device.type == 'cuda':\n",
    "            for event in key_averages:\n",
    "                if hasattr(event, 'self_cuda_time_total') and event.self_cuda_time_total > 0: has_cuda_events = True; break\n",
    "            if has_cuda_events: sort_key = \"self_cuda_time_total\"\n",
    "            else: print(\"No significant CUDA activity in profiler, sorting by CPU time for console.\")\n",
    "        \n",
    "        # Log profiler table to wandb\n",
    "        try:\n",
    "            table_data = []; columns = [\"Operator\", \"Self CPU %\", \"Self CPU\", \"CPU total %\", \"CPU total\", \"Calls\"]\n",
    "            if has_cuda_events: columns.extend([\"Self CUDA %\", \"Self CUDA\", \"CUDA total %\", \"CUDA total\"])\n",
    "            \n",
    "            total_cpu_time = sum(evt.cpu_time for evt in key_averages) # Or use prof.total_average().cpu_time\n",
    "            total_cuda_time = sum(evt.cuda_time for evt in key_averages if hasattr(evt, 'cuda_time')) if has_cuda_events else 0\n",
    "\n",
    "            for ev_avg in sorted(key_averages, key=lambda x: getattr(x, sort_key, 0), reverse=True):\n",
    "                row = [\n",
    "                    ev_avg.key,\n",
    "                    f\"{ev_avg.self_cpu_time_percent:.2f}%\", f\"{ev_avg.self_cpu_time_total/1000:.2f}ms\",\n",
    "                    f\"{ev_avg.cpu_time_percent:.2f}%\", f\"{ev_avg.cpu_time_total/1000:.2f}ms\",\n",
    "                    ev_avg.count\n",
    "                ]\n",
    "                if has_cuda_events:\n",
    "                    row.extend([\n",
    "                        f\"{getattr(ev_avg, 'self_cuda_time_percent', 0.0):.2f}%\", f\"{getattr(ev_avg, 'self_cuda_time_total', 0)/1000:.2f}ms\",\n",
    "                        f\"{getattr(ev_avg, 'cuda_time_percent', 0.0):.2f}%\", f\"{getattr(ev_avg, 'cuda_time_total', 0)/1000:.2f}ms\"\n",
    "                    ])\n",
    "                table_data.append(row)\n",
    "            if table_data:\n",
    "                profiler_wandb_table = wandb.Table(columns=columns, data=table_data)\n",
    "                wandb.log({\"pytorch_profiler_summary\": profiler_wandb_table})\n",
    "                print(\"Logged PyTorch Profiler summary as a wandb.Table.\")\n",
    "        except Exception as e: print(f\"Error creating/logging wandb.Table for profiler: {e}\")\n",
    "        \n",
    "        print(f\"Console Profiler Table (sorted by {sort_key}):\")\n",
    "        print(key_averages.table(sort_by=sort_key, row_limit=15))\n",
    "\n",
    "        if export_chrome_trace:\n",
    "            trace_file_name = f\"gigagan_trace_{gan.device.type}_{wandb.run.id}.json\"\n",
    "            trace_file_path = os.path.join(profile_log_dir, trace_file_name)\n",
    "            try:\n",
    "                prof.export_chrome_trace(trace_file_path)\n",
    "                print(f\"\\nChrome trace file saved to: {trace_file_path}\")\n",
    "                profile_artifact = wandb.Artifact(name=f\"profiler-trace-{wandb.run.id}\", type=\"profile\")\n",
    "                profile_artifact.add_file(trace_file_path)\n",
    "                wandb.log_artifact(profile_artifact)\n",
    "                print(f\"Logged Chrome trace artifact to wandb: {profile_artifact.name}\")\n",
    "            except Exception as e: print(f\"\\nCould not export or log Chrome trace: {e}\")\n",
    "    elif prof: print(\"\\n--- PyTorch Profiler Results (wandb run not active, printing to console only) ---\") ; print(prof.key_averages().table(row_limit=15))\n",
    "    else: print(\"Profiler was not active or did not record data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a44d499c-8fad-4c7a-a4cd-14ec324733c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250509_172908-rtt24nj3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments/runs/rtt24nj3' target=\"_blank\">GigaGAN_QProf_in32_out64_cuda_13mz</a></strong> to <a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments' target=\"_blank\">https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments/runs/rtt24nj3' target=\"_blank\">https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments/runs/rtt24nj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb run initialized: GigaGAN_QProf_in32_out64_cuda_13mz (ID: rtt24nj3)\n",
      "Target Device: cuda\n",
      "\n",
      "Loading model...\n",
      "Initializing GigaGANQuantizedV100 on device: cuda\n",
      "Setting D multiscale_input_resolutions to: [32]\n",
      "\n",
      "\n",
      "Generator: 43.71M\n",
      "Discriminator: 30.74M\n",
      "\n",
      "\n",
      "Model G device after .to(): cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gigagan_pytorch/gigagan_pytorch.py:1968: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pkg = torch.load(str(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully called gan.load(./model-32.ckpt)\n",
      "Starting quantization for V100 GPU...\n",
      "Generator 'G' target device for quantization: cuda\n",
      "Generator 'G' current device: cuda:0\n",
      "Model G converted to FP16.\n",
      "Critical components preservation step complete.\n",
      "No eligible linear layers found or converted to INT8.\n",
      "Selective INT8 quantization step complete.\n",
      "Stability hooks step complete.\n",
      "Quantization complete!\n",
      "Model is in evaluation mode.\n",
      "\n",
      "Starting image generation and profiling...\n",
      "Generating images on device: cuda\n",
      "Model type: Upsampler/Conditional. Dummy input size: (64, 64).\n",
      "Dummy input tensor: shape torch.Size([1, 3, 64, 64]), dtype torch.float16\n",
      "\n",
      "Starting profiling session (wait=1, warmup=1, active=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_551/4236201076.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/tmp/ipykernel_551/4236201076.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "[W509 17:29:12.148718107 CPUAllocator.cpp:249] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished generating 5 images (local saving commented out during profiling).\n",
      "\n",
      "--- PyTorch Profiler Results (logged to wandb) ---\n",
      "No significant CUDA activity in profiler, sorting by CPU time for console.\n",
      "Error creating/logging wandb.Table for profiler: 'FunctionEventAvg' object has no attribute 'self_cpu_time_percent'\n",
      "Console Profiler Table (sorted by self_cpu_time_total):\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        18.24%      34.012ms        18.24%      34.012ms       6.926us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          4911            --  \n",
      "                                          ProfilerStep*        16.53%      30.829ms        20.53%      38.286ms      12.762ms       0.000us         0.00%       1.646ms     548.639us     768.00 Kb      -3.19 Mb    -548.58 Mb    -709.13 Mb             3            --  \n",
      "                                aten::cudnn_convolution         7.02%      13.081ms        10.62%      19.796ms      55.920us      11.191ms        17.15%      11.191ms      31.614us           0 b           0 b       1.18 Gb       1.18 Gb           354            --  \n",
      "                                    aten::empty_strided         5.03%       9.373ms         5.03%       9.373ms       8.801us       0.000us         0.00%       0.000us       0.000us       1.69 Mb       1.69 Mb       1.24 Gb       1.24 Gb          1065            --  \n",
      "                                            aten::copy_         4.85%       9.044ms        10.65%      19.853ms      17.278us       9.744ms        14.94%       9.744ms       8.480us           0 b           0 b           0 b           0 b          1149            --  \n",
      "                                              aten::mul         4.64%       8.653ms         8.20%      15.283ms      22.811us       6.875ms        10.54%       6.875ms      10.261us     768.00 Kb     768.00 Kb       1.63 Gb       1.63 Gb           670       430.597  \n",
      "                                              aten::sum         3.31%       6.164ms         4.60%       8.572ms      28.013us       3.479ms         5.33%       3.479ms      11.370us           0 b           0 b     217.03 Mb     217.03 Mb           306            --  \n",
      "                                           aten::conv2d         3.07%       5.716ms        47.07%      87.757ms     118.911us       0.000us         0.00%      30.254ms      40.995us           0 b           0 b       2.37 Gb    -456.08 Mb           738    260309.358  \n",
      "                                         aten::_to_copy         2.78%       5.192ms        17.52%      32.664ms      30.700us       0.000us         0.00%       9.173ms       8.621us     960.01 Kb           0 b       1.24 Gb           0 b          1064            --  \n",
      "                                            aten::empty         2.47%       4.607ms         2.47%       4.607ms       5.212us       0.000us         0.00%       0.000us       0.000us         248 b         248 b     285.14 Mb     285.14 Mb           884            --  \n",
      "                                aten::native_group_norm         2.36%       4.401ms         5.67%      10.566ms      76.563us       7.632ms        11.70%       7.632ms      55.302us           0 b           0 b     223.63 Mb    -198.00 Kb           138            --  \n",
      "                                              aten::add         1.88%       3.513ms         2.94%       5.486ms      18.852us       1.778ms         2.73%       1.778ms       6.110us           0 b           0 b     202.20 Mb     202.20 Mb           291        53.695  \n",
      "                                        cudaMemcpyAsync         1.77%       3.299ms         1.77%       3.299ms     206.202us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16            --  \n",
      "                                     aten::_convolution         1.72%       3.209ms        14.54%      27.107ms      73.461us       0.000us         0.00%      12.683ms      34.371us           0 b           0 b       1.18 Gb      -7.69 Mb           369            --  \n",
      "                                          aten::reshape         1.55%       2.889ms         2.58%       4.813ms       3.450us       0.000us         0.00%      42.747us       0.031us           0 b           0 b       5.25 Mb           0 b          1395            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 186.455ms\n",
      "Self CUDA time total: 65.238ms\n",
      "\n",
      "\n",
      "Chrome trace file saved to: ./profiler_wandb_logs/gigagan_trace_cuda_rtt24nj3.json\n",
      "Logged Chrome trace artifact to wandb: profiler-trace-rtt24nj3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GigaGAN_QProf_in32_out64_cuda_13mz</strong> at: <a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments/runs/rtt24nj3' target=\"_blank\">https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments/runs/rtt24nj3</a><br> View project at: <a href='https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments' target=\"_blank\">https://wandb.ai/sb9855-new-york-university/gigagan-profiling-experiments</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb run finished.\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE_STR = \"cuda\" if USE_CUDA else \"cpu\" \n",
    "DEVICE = torch.device(DEVICE_STR)\n",
    "\n",
    "DUMMY_CHECKPOINT_PATH = \"./model-32.ckpt\" # Matching user's previous log\n",
    "if not os.path.exists(DUMMY_CHECKPOINT_PATH):\n",
    "    print(f\"Creating dummy checkpoint: {DUMMY_CHECKPOINT_PATH}\")\n",
    "    torch.save({\"generator\": {}, \"discriminator\": {}}, DUMMY_CHECKPOINT_PATH)\n",
    "\n",
    "# --- Parameters ---\n",
    "CHECKPOINT_TO_LOAD = DUMMY_CHECKPOINT_PATH\n",
    "NUM_IMAGES_TO_GENERATE = 5 \n",
    "OUTPUT_IMAGE_DIR = \"generated_output_wandb\" # Local output if any\n",
    "PROFILE_LOG_DIR = './profiler_wandb_logs' \n",
    "EXPORT_CHROME_TRACE_FILE = True \n",
    "\n",
    "IMAGE_SIZE = 64 # Final image size\n",
    "INPUT_IMAGE_SIZE = 32 # Input to upsampler / base size for profiler error\n",
    "UNCONDITIONAL_GAN = True\n",
    "\n",
    "# --- Initialize W&B Run ---\n",
    "run_name = f\"GigaGAN_QProf_in{INPUT_IMAGE_SIZE}_out{IMAGE_SIZE}_{DEVICE_STR}_{wandb.util.generate_id()[:4]}\"\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=\"gigagan-profiling-experiments\", # REPLACE with your project name\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"checkpoint\": CHECKPOINT_TO_LOAD, \"num_images\": NUM_IMAGES_TO_GENERATE,\n",
    "            \"image_size\": IMAGE_SIZE, \"input_image_size\": INPUT_IMAGE_SIZE,\n",
    "            \"unconditional_gan\": UNCONDITIONAL_GAN, \"device\": DEVICE_STR,\n",
    "            \"quantization_target\": \"V100_style_mixed_precision\", \"profiling_enabled\": True,\n",
    "            \"export_chrome_trace\": EXPORT_CHROME_TRACE_FILE\n",
    "        },\n",
    "        # mode=\"offline\" # Use if you don't have internet access during run\n",
    "    )\n",
    "    print(f\"Wandb run initialized: {wandb.run.name if wandb.run else 'FAILED'} (ID: {wandb.run.id if wandb.run else 'N/A'})\")\n",
    "    print(f\"Target Device: {DEVICE}\")\n",
    "\n",
    "    # --- Run ---\n",
    "    print(\"\\nLoading model...\")\n",
    "    gan_model = load_quantized_gigagan(\n",
    "        checkpoint_path=CHECKPOINT_TO_LOAD, device_str=DEVICE_STR,\n",
    "        unconditional_gan=UNCONDITIONAL_GAN, image_size=IMAGE_SIZE,\n",
    "        input_image_size=INPUT_IMAGE_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStarting image generation and profiling...\")\n",
    "    generate_images_and_profile(\n",
    "        gan_model, num_images=NUM_IMAGES_TO_GENERATE, output_dir=OUTPUT_IMAGE_DIR,\n",
    "        profile_log_dir=PROFILE_LOG_DIR, export_chrome_trace=EXPORT_CHROME_TRACE_FILE\n",
    "    )\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    if wandb.run: wandb.log({\"error_type\": \"ImportError\", \"error_message\": str(e)})\n",
    "except Exception as e:\n",
    "    tb_str = traceback.format_exc()\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Traceback:\"); print(tb_str)\n",
    "    if wandb.run: wandb.log({\"error_type\": str(type(e).__name__), \"error_message\": str(e), \"traceback\": tb_str})\n",
    "finally:\n",
    "    if wandb.run: # Ensure wandb.run exists before trying to finish\n",
    "        wandb.finish() \n",
    "        print(\"Wandb run finished.\")\n",
    "    else:\n",
    "        print(\"Wandb run was not initialized or failed to initialize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2abb1-138f-430b-8d7d-52a21e9ae003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
