{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install required packages\n",
        "!pip install datasets huggingface_hub torch torchvision validators transformers fvcore bitsandbytes torch-fidelity einops\n",
        "\n",
        "# Cell 2: Install GigaGAN package\n",
        "!pip install gigagan-pytorch==0.2.20\n",
        "\n",
        "# Cell 3: Import libraries, Setup PROFILING_DIR and Logger\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from __future__ import annotations # Should be at the very top if used\n",
        "from gigagan_pytorch import GigaGAN\n",
        "from huggingface_hub import login\n",
        "from PIL import Image, ImageFile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import validators\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import torch.nn as nn\n",
        "# from torch.cuda.amp import autocast, GradScaler # Deprecated\n",
        "from torch.amp import autocast, GradScaler # Corrected import\n",
        "import bitsandbytes as bnb\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import torch_fidelity\n",
        "import torch.nn.functional as F # For KD loss\n",
        "\n",
        "# Import profiling tools\n",
        "import torch.profiler as profiler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table, parameter_count\n",
        "\n",
        "# Create profiling directory\n",
        "PROFILING_DIR = \"profiling_results_kd\"\n",
        "os.makedirs(PROFILING_DIR, exist_ok=True)\n",
        "\n",
        "# Setup logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(os.path.join(PROFILING_DIR,'gigagan_kd_profiling.log')),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Knowledge Distillation GigaGAN Notebook Started\")\n",
        "\n",
        "# Cell 4: One time function execution (commented out)\n",
        "# ... (same as in original notebook) ...\n",
        "\n",
        "# Cell 5: Image loading utilities\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "def safe_getitem(self, index):\n",
        "    max_attempts = 5; current_index = index\n",
        "    for _ in range(max_attempts):\n",
        "        try:\n",
        "            path = self.paths[current_index]; img = Image.open(path); img.load()\n",
        "            if img.mode != 'RGB': img = img.convert('RGB')\n",
        "            return self.transform(img)\n",
        "        except (OSError, IOError, SyntaxError) as e:\n",
        "            logger.warning(f\"Error loading image {path}: {e}. Trying next.\"); current_index = (current_index + 1) % len(self.paths)\n",
        "            if current_index == index: logger.error(\"Looped all images.\"); break\n",
        "    logger.error(f\"Failed {max_attempts} loads, returning blank.\"); return torch.zeros(3, self.image_size, self.image_size)\n",
        "\n",
        "def patch_image_dataset(dataset):\n",
        "    ImageFile.LOAD_TRUNCATED_IMAGES = True; dataset.__class__.__getitem__ = safe_getitem; return dataset\n",
        "\n",
        "# Cell 6: Unzip dataset (if applicable)\n",
        "dataset_zip_path = '/content/final_dataset.zip'; dataset_extract_path = '/content/final_dataset'\n",
        "!unzip -q /content/final_dataset.zip -d /content/final_dataset\n",
        "if os.path.exists(dataset_zip_path) and not os.path.exists(os.path.join(dataset_extract_path, \"10047.png\")):\n",
        "    logger.info(f\"Unzipping {dataset_zip_path} to {dataset_extract_path}...\")\n",
        "    os.makedirs(dataset_extract_path, exist_ok=True)\n",
        "    # !unzip -q /content/final_dataset.zip -d /content/final_dataset\n",
        "    logger.info(\"Dataset unzipped (command executed).\")\n",
        "else:\n",
        "    if not os.path.exists(dataset_zip_path): logger.warning(f\"Dataset zip {dataset_zip_path} not found.\")\n",
        "    else: logger.info(f\"Dataset seems unzipped in {dataset_extract_path}.\")\n",
        "\n",
        "# Cell 7: Load Dataset\n",
        "from gigagan_pytorch import ImageDataset\n",
        "dataset = None; dataloader = None\n",
        "DATASET_PATH = '/content/final_dataset'; IMAGE_SIZE = 256; BATCH_SIZE = 4 # KD Batch size will be set later\n",
        "try:\n",
        "    if not os.path.isdir(DATASET_PATH) or not os.listdir(DATASET_PATH): raise FileNotFoundError(f\"Dataset dir {DATASET_PATH} missing/empty.\")\n",
        "    dataset = ImageDataset(folder=DATASET_PATH, image_size=IMAGE_SIZE)\n",
        "    if len(dataset) == 0: raise ValueError(\"Dataset empty post-init.\")\n",
        "    dataset = patch_image_dataset(dataset)\n",
        "    # Dataloader for KD will be created specifically in the KD cell with its own batch size.\n",
        "    # However, having a general dataloader for other potential uses (like initial teacher check) is fine.\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    logger.info(f\"Loaded dataset: {len(dataset)} images. Default dataloader batch: {BATCH_SIZE}.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load dataset from {DATASET_PATH}: {e}\", exc_info=True)\n",
        "    logger.warning(\"Creating dummy dataset.\");\n",
        "    class DummyDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, size=100, image_size=256): self.size=size; self.image_size=image_size; self.transform=transforms.Compose([transforms.ToTensor()])\n",
        "        def __len__(self): return self.size\n",
        "        def __getitem__(self, idx): return self.transform(Image.new('RGB', (self.image_size,self.image_size),color='gray'))\n",
        "    dataset = DummyDataset(image_size=IMAGE_SIZE); dataloader = DataLoader(dataset, batch_size=BATCH_SIZE) # Dataloader for KD will be specific\n",
        "    logger.info(f\"Using dummy dataset: {len(dataset)} images.\")\n",
        "\n",
        "\n",
        "# Cell 8: Teacher Model Setup (from Original GigaGAN)\n",
        "logger.info(\"--- Setting up Teacher Model for Knowledge Distillation ---\")\n",
        "teacher_model = None\n",
        "teacher_ckpt_path = '/content/model-32.ckpt' # This is the original, pre-trained model\n",
        "\n",
        "# Define teacher model configuration (should match the checkpoint)\n",
        "teacher_generator_config = dict(\n",
        "    style_network=dict(dim=64, depth=4), dim=32, image_size=256,\n",
        "    input_image_size=64, unconditional=True, flash_attn=False\n",
        ")\n",
        "teacher_discriminator_config = dict(\n",
        "    dim_capacity=16, dim_max=512, image_size=256, num_skip_layers_excite=4,\n",
        "    multiscale_input_resolutions=(128,), unconditional=True\n",
        ")\n",
        "# Set amp based on how the teacher_ckpt_path was saved.\n",
        "teacher_model_amp_setting = False # Assuming model-32.ckpt was saved with amp=False\n",
        "\n",
        "try:\n",
        "    logger.info(f\"Initializing Teacher GigaGAN with AMP={teacher_model_amp_setting}...\")\n",
        "    teacher_model = GigaGAN(\n",
        "        train_upsampler=True,\n",
        "        generator=teacher_generator_config,\n",
        "        discriminator=teacher_discriminator_config,\n",
        "        amp=teacher_model_amp_setting\n",
        "    ).cuda()\n",
        "    logger.info(\"Teacher GigaGAN structure initialized.\")\n",
        "\n",
        "    if os.path.exists(teacher_ckpt_path):\n",
        "        teacher_model.load(teacher_ckpt_path)\n",
        "        logger.info(f\"Loaded teacher model checkpoint from {teacher_ckpt_path}\")\n",
        "    else:\n",
        "        logger.warning(f\"Teacher checkpoint not found at {teacher_ckpt_path}. KD will use initialized teacher weights.\")\n",
        "\n",
        "    teacher_model.eval() # Set teacher to evaluation mode\n",
        "    for param in teacher_model.parameters(): # Freeze teacher weights\n",
        "        param.requires_grad = False\n",
        "    logger.info(\"Teacher model weights frozen and set to eval mode.\")\n",
        "\n",
        "except Exception as e_teacher:\n",
        "    logger.error(f\"Major error setting up teacher model: {e_teacher}\", exc_info=True)\n",
        "    teacher_model = None\n",
        "\n",
        "\n",
        "# Cell 9: Student Model Definition\n",
        "logger.info(\"--- Defining Student Model for Knowledge Distillation ---\")\n",
        "gan_student = None\n",
        "if teacher_model is not None:\n",
        "    try:\n",
        "        student_generator_config = dict(\n",
        "            style_network=dict(dim=32, depth=3), # Reduced\n",
        "            dim=16, # Reduced\n",
        "            image_size=256, input_image_size=64, # Match teacher I/O\n",
        "            unconditional=True, flash_attn=False\n",
        "        )\n",
        "        student_discriminator_config = dict(\n",
        "            dim_capacity=8, dim_max=256, # Reduced\n",
        "            image_size=256, num_skip_layers_excite=2, # Reduced\n",
        "            multiscale_input_resolutions=(128,), unconditional=True\n",
        "        )\n",
        "        # AMP setting for student model during KD training.\n",
        "        student_initial_amp = False # If teacher_model_amp_setting was True, this should also be True.\n",
        "\n",
        "        logger.info(f\"Initializing Student GigaGAN with AMP={student_initial_amp}...\")\n",
        "        gan_student = GigaGAN(\n",
        "            train_upsampler=True, # Match teacher\n",
        "            generator=student_generator_config,\n",
        "            discriminator=student_discriminator_config,\n",
        "            amp=student_initial_amp\n",
        "        ).cuda()\n",
        "        logger.info(f\"Student GigaGAN model structure initialized (untrained).\")\n",
        "    except Exception as e_student_def:\n",
        "        logger.error(f\"Failed to define student model: {e_student_def}\", exc_info=True)\n",
        "        gan_student = None\n",
        "else:\n",
        "    logger.error(\"Teacher model not available. Cannot define student model.\")\n",
        "\n",
        "\n",
        "# Cell 10: Knowledge Distillation Training (Conceptual)\n",
        "# This cell uses placeholder GAN losses and focuses on the distillation aspect.\n",
        "\n",
        "logger.info(\"--- Conceptual Knowledge Distillation Training Setup ---\")\n",
        "\n",
        "if 'teacher_model' in locals() and teacher_model is not None and \\\n",
        "   'gan_student' in locals() and gan_student is not None and \\\n",
        "   'dataset' in locals() and dataset is not None: # Check for dataset\n",
        "\n",
        "    logger.info(\"Teacher, Student, and Dataset found. Setting up KD parameters.\")\n",
        "    logger.warning(\"NOTE: This loop uses PLACEHOLDER GAN losses and is conceptual.\")\n",
        "    logger.warning(\"Actual KD training requires implementing real GAN loss calculations.\")\n",
        "\n",
        "\n",
        "    KD_TRAINING_STEPS = 100 # Reduced for quick test\n",
        "    KD_BATCH_SIZE = 4      # Batch size for KD training\n",
        "    KD_LR_G = 1e-4\n",
        "    KD_LR_D = 1e-4\n",
        "    LAMBDA_KD = 10.0\n",
        "    KD_GRAD_ACCUM_EVERY = 1\n",
        "    KD_LOG_EVERY = 20\n",
        "    KD_SAVE_EVERY = 50\n",
        "\n",
        "    # Create a new dataloader instance for KD\n",
        "    kd_dataloader = DataLoader(dataset, batch_size=KD_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    logger.info(f\"Created KD Dataloader with batch size {KD_BATCH_SIZE}.\")\n",
        "\n",
        "\n",
        "    student_optimizer_G = torch.optim.Adam(gan_student.G.parameters(), lr=KD_LR_G, betas=(0.9, 0.99))\n",
        "    student_optimizer_D = torch.optim.Adam(gan_student.D.parameters(), lr=KD_LR_D, betas=(0.9, 0.99))\n",
        "\n",
        "    # Student's accelerator state for GradScaler and autocast\n",
        "    is_student_amp_fp16_kd = (gan_student.accelerator.state.mixed_precision == 'fp16')\n",
        "    logger.info(f\"Student model KD training AMP (fp16) status: {is_student_amp_fp16_kd}\")\n",
        "    student_scaler_G = torch.amp.GradScaler(device='cuda', enabled=is_student_amp_fp16_kd)\n",
        "    student_scaler_D = torch.amp.GradScaler(device='cuda', enabled=is_student_amp_fp16_kd)\n",
        "\n",
        "    gan_student.train() # Student in training mode\n",
        "    teacher_model.eval() # Teacher stays in eval\n",
        "\n",
        "    logger.info(f\"Starting conceptual KD training for {KD_TRAINING_STEPS} steps...\")\n",
        "\n",
        "    student_gen_input_size = gan_student.unwrapped_G.input_image_size\n",
        "    resize_transform_student_kd = T.Resize((student_gen_input_size, student_gen_input_size), antialias=True)\n",
        "\n",
        "    style_dim_student_kd = gan_student.unwrapped_G.style_network.dim\n",
        "    style_dim_teacher_kd = teacher_model.unwrapped_G.style_network.dim\n",
        "\n",
        "    teacher_gen_input_size_kd = teacher_model.unwrapped_G.input_image_size\n",
        "    resize_transform_teacher_kd = None\n",
        "    dataloader_image_size = IMAGE_SIZE # Get size from dataset config used earlier\n",
        "    if teacher_gen_input_size_kd != dataloader_image_size:\n",
        "        resize_transform_teacher_kd = T.Resize((teacher_gen_input_size_kd, teacher_gen_input_size_kd), antialias=True)\n",
        "\n",
        "    kd_dataloader_iter = iter(kd_dataloader)\n",
        "    current_device_kd = gan_student.accelerator.device\n",
        "    teacher_device_kd = teacher_model.accelerator.device\n",
        "    is_teacher_amp_fp16_kd = (teacher_model.accelerator.state.mixed_precision == 'fp16')\n",
        "\n",
        "    for step in range(KD_TRAINING_STEPS):\n",
        "        try:\n",
        "            try:\n",
        "                real_images_full_res = next(kd_dataloader_iter)\n",
        "                if isinstance(real_images_full_res, (list, tuple)): real_images_full_res = real_images_full_res[0]\n",
        "            except StopIteration:\n",
        "                logger.info(f\"KD Dataloader exhausted at step {step}. Resetting.\")\n",
        "                kd_dataloader_iter = iter(kd_dataloader); real_images_full_res = next(kd_dataloader_iter)\n",
        "                if isinstance(real_images_full_res, (list, tuple)): real_images_full_res = real_images_full_res[0]\n",
        "\n",
        "            real_images_full_res = real_images_full_res.to(current_device_kd)\n",
        "            current_batch_size_dynamic = real_images_full_res.size(0)\n",
        "            if current_batch_size_dynamic == 0: logger.warning(\"KD: Fetched empty batch.\"); continue\n",
        "\n",
        "            # Student's low-resolution input\n",
        "            lowres_input_for_student_kd = resize_transform_student_kd(real_images_full_res)\n",
        "\n",
        "            # Teacher's low-resolution input\n",
        "            if resize_transform_teacher_kd:\n",
        "                lowres_input_for_teacher_kd = resize_transform_teacher_kd(real_images_full_res)\n",
        "            elif teacher_gen_input_size_kd == real_images_full_res.shape[-1]:\n",
        "                lowres_input_for_teacher_kd = real_images_full_res.clone()\n",
        "            else:\n",
        "                lowres_input_for_teacher_kd = lowres_input_for_student_kd.clone()\n",
        "\n",
        "            noise_batch_student_kd = torch.randn(current_batch_size_dynamic, style_dim_student_kd, device=current_device_kd)\n",
        "            noise_batch_teacher_kd = torch.randn(current_batch_size_dynamic, style_dim_teacher_kd, device=teacher_device_kd)\n",
        "\n",
        "            student_input_processed_G_kd = lowres_input_for_student_kd\n",
        "            teacher_input_processed_G_kd = lowres_input_for_teacher_kd.to(teacher_device_kd)\n",
        "\n",
        "            # --- Update Student Discriminator (Conceptual) ---\n",
        "            student_optimizer_D.zero_grad(set_to_none=True)\n",
        "            for _ in range(KD_GRAD_ACCUM_EVERY):\n",
        "                # Placeholder loss - does not require model call or autocast\n",
        "                d_loss_placeholder = torch.tensor(1.0, device=current_device_kd, requires_grad=True)\n",
        "                d_loss_accum = d_loss_placeholder / KD_GRAD_ACCUM_EVERY\n",
        "                # Need to scale even placeholder loss if scaler is enabled\n",
        "                student_scaler_D.scale(d_loss_accum).backward()\n",
        "            student_scaler_D.unscale_(student_optimizer_D)\n",
        "            student_scaler_D.step(student_optimizer_D)\n",
        "            student_scaler_D.update()\n",
        "\n",
        "            # --- Update Student Generator (Conceptual GAN Loss + Real Distillation Loss) ---\n",
        "            student_optimizer_G.zero_grad(set_to_none=True)\n",
        "            for _ in range(KD_GRAD_ACCUM_EVERY):\n",
        "                with torch.amp.autocast(device_type=current_device_kd.type, dtype=torch.float16, enabled=is_student_amp_fp16_kd):\n",
        "                    # Generate student images (needed for distillation loss)\n",
        "                    fake_images_student = gan_student.G(\n",
        "                        student_input_processed_G_kd,\n",
        "                        noise=noise_batch_student_kd # G takes noise as keyword\n",
        "                    )\n",
        "\n",
        "                    # Placeholder GAN loss for generator\n",
        "                    g_loss_gan_placeholder = torch.tensor(0.5, device=current_device_kd) # No grad needed for placeholder\n",
        "\n",
        "                    # Generate teacher images (still needed for distillation)\n",
        "                    with torch.no_grad():\n",
        "                        with torch.amp.autocast(device_type=teacher_device_kd.type, dtype=torch.float16, enabled=is_teacher_amp_fp16_kd):\n",
        "                            fake_images_teacher = teacher_model.G(\n",
        "                                teacher_input_processed_G_kd,\n",
        "                                noise=noise_batch_teacher_kd\n",
        "                            ).detach()\n",
        "\n",
        "                    # Calculate REAL distillation loss\n",
        "                    distill_loss = F.mse_loss(fake_images_student.float(), fake_images_teacher.to(fake_images_student.device).float())\n",
        "\n",
        "                    # Combine placeholder GAN loss and real distillation loss\n",
        "                    # Only distill_loss requires grad here\n",
        "                    total_g_loss = g_loss_gan_placeholder + (LAMBDA_KD * distill_loss)\n",
        "                    total_g_loss_accum = total_g_loss / KD_GRAD_ACCUM_EVERY\n",
        "\n",
        "                # Scale and backward pass for G (gradient comes only from distill_loss)\n",
        "                student_scaler_G.scale(total_g_loss_accum).backward()\n",
        "            student_scaler_G.unscale_(student_optimizer_G)\n",
        "            student_scaler_G.step(student_optimizer_G)\n",
        "            student_scaler_G.update()\n",
        "\n",
        "            if step % KD_LOG_EVERY == 0:\n",
        "                 # Log placeholder and real loss values\n",
        "                 d_loss_item = d_loss_placeholder.item()\n",
        "                 g_loss_gan_item = g_loss_gan_placeholder.item()\n",
        "                 distill_loss_item = distill_loss.item() if torch.is_tensor(distill_loss) else float('nan')\n",
        "                 total_g_loss_item = total_g_loss.item() if torch.is_tensor(total_g_loss) else float('nan')\n",
        "                 logger.info(f\"KD Step [{step}/{KD_TRAINING_STEPS}] | D Loss (PH): {d_loss_item:.4f} | G Loss (PH): {g_loss_gan_item:.4f} | Distill Loss: {distill_loss_item:.4f} | Total G Loss: {total_g_loss_item:.4f}\")\n",
        "\n",
        "            if step > 0 and (step % KD_SAVE_EVERY == 0 or step == KD_TRAINING_STEPS - 1):\n",
        "                 student_save_path = os.path.join(PROFILING_DIR, f'student_model_step_{step}.ckpt')\n",
        "                 try:\n",
        "                     gan_student.save(student_save_path)\n",
        "                     logger.info(f\"Student checkpoint saved to {student_save_path}\")\n",
        "                 except Exception as save_e:\n",
        "                     logger.error(f\"Failed to save student checkpoint at step {step}: {save_e}\")\n",
        "\n",
        "        except Exception as train_err:\n",
        "            logger.error(f\"Error during conceptual KD step {step}: {train_err}\", exc_info=True)\n",
        "            if step > 0 :\n",
        "                student_error_save_path = os.path.join(PROFILING_DIR, f'student_model_error_step_{step}.ckpt')\n",
        "                try: gan_student.save(student_error_save_path); logger.info(f\"Saved error checkpoint: {student_error_save_path}\")\n",
        "                except Exception as save_err: logger.error(f\"Could not save error checkpoint: {save_err}\")\n",
        "            break\n",
        "\n",
        "    logger.info(f\"Conceptual KD training finished or stopped at step {step}.\")\n",
        "    student_final_ckpt_path = os.path.join(PROFILING_DIR, 'student_model_final.ckpt')\n",
        "    # Check if final save is needed (if last step wasn't a save step)\n",
        "    if step == KD_TRAINING_STEPS - 1 and KD_TRAINING_STEPS > 0 and KD_TRAINING_STEPS % KD_SAVE_EVERY != 0 :\n",
        "        try: gan_student.save(student_final_ckpt_path); logger.info(f\"Final student model saved to {student_final_ckpt_path}\")\n",
        "        except Exception as e: logger.error(f\"Could not save final student model: {e}\")\n",
        "\n",
        "    logger.warning(\"This was conceptual training. Load a properly trained student checkpoint before profiling.\")\n",
        "else:\n",
        "    # Check which condition failed\n",
        "    if 'teacher_model' not in locals() or teacher_model is None:\n",
        "        logger.error(\"KD Training cannot start: Teacher model is missing/invalid.\")\n",
        "    elif 'gan_student' not in locals() or gan_student is None:\n",
        "        logger.error(\"KD Training cannot start: Student model is missing/invalid.\")\n",
        "    elif 'dataset' not in locals() or dataset is None: # Changed check to dataset\n",
        "        logger.error(\"KD Training cannot start: Dataset is missing/invalid.\")\n",
        "    else: # Should not happen if one of the above is true\n",
        "         logger.error(\"KD Training cannot start: Unknown reason (Teacher, Student, or Dataset missing/invalid).\")\n",
        "\n",
        "\n",
        "# Cell 11: Trained Student GigaGAN Model Analysis and Profiling\n",
        "# ... (rest of the notebook remains the same, but will profile the conceptually trained model if run directly after Cell 10) ...\n",
        "# Cell 11: Trained Student GigaGAN Model Analysis and Profiling\n",
        "logger.info(\"--- Trained Student Model Analysis and Profiling ---\")\n",
        "# This cell assumes gan_student is either still in memory from KD or loaded from a checkpoint.\n",
        "# For a clean run, it's better to explicitly load the desired student checkpoint.\n",
        "\n",
        "gan_student_profiling = None # Use a new variable for clarity\n",
        "# Point to the checkpoint saved from the *conceptual* training if you want to profile that.\n",
        "# Otherwise, point to a checkpoint from a *real* training run.\n",
        "student_checkpoint_to_profile = os.path.join(PROFILING_DIR, 'student_model_final.ckpt')\n",
        "\n",
        "if os.path.exists(student_checkpoint_to_profile):\n",
        "    logger.info(f\"Loading student model from {student_checkpoint_to_profile} for profiling...\")\n",
        "    try:\n",
        "        if 'student_generator_config' not in globals() or 'student_discriminator_config' not in globals():\n",
        "            raise NameError(\"Student model configurations not found. Cannot reload student model.\")\n",
        "        student_load_amp = False # Match the amp setting used when saving\n",
        "        gan_student_profiling = GigaGAN(\n",
        "            train_upsampler=True, generator=student_generator_config,\n",
        "            discriminator=student_discriminator_config, amp=student_load_amp\n",
        "        ).cuda()\n",
        "        gan_student_profiling.load(student_checkpoint_to_profile)\n",
        "        logger.info(\"Student model loaded successfully for profiling.\")\n",
        "        gan_student_profiling.eval()\n",
        "    except Exception as e_load_stud:\n",
        "        logger.error(f\"Failed to load student model {student_checkpoint_to_profile} for profiling: {e_load_stud}\", exc_info=True)\n",
        "        gan_student_profiling = None\n",
        "else:\n",
        "    logger.warning(f\"Student checkpoint {student_checkpoint_to_profile} not found. Profiling will be skipped or use in-memory model if available.\")\n",
        "    if 'gan_student' in locals() and gan_student is not None:\n",
        "        logger.info(\"Using in-memory 'gan_student' (potentially only conceptually trained) for profiling.\")\n",
        "        gan_student_profiling = gan_student\n",
        "        gan_student_profiling.eval()\n",
        "    else:\n",
        "        gan_student_profiling = None\n",
        "\n",
        "if gan_student_profiling is not None:\n",
        "    current_device_stud = gan_student_profiling.accelerator.device\n",
        "    architecture_stats_student = {\"model_type\": \"student_kd_conceptual\"} # Note conceptual\n",
        "    try:\n",
        "        gen_params_s = sum(p.numel() for p in gan_student_profiling.G.parameters())\n",
        "        disc_params_s = sum(p.numel() for p in gan_student_profiling.D.parameters()); total_params_s = gen_params_s + disc_params_s\n",
        "        model_size_mb_s = sum(p.numel()*p.element_size() for p in gan_student_profiling.parameters())/ (1024*1024)\n",
        "        architecture_stats_student.update({\n",
        "            \"gpu_type\": \"A100\" if current_device_stud.type == 'cuda' else \"CPU\",\n",
        "            \"amp_setting\": gan_student_profiling.accelerator.state.mixed_precision,\n",
        "            \"generator_parameters\": gen_params_s, \"discriminator_parameters\": disc_params_s, \"total_parameters\": total_params_s,\n",
        "            \"model_size_mb\": model_size_mb_s\n",
        "        })\n",
        "        logger.info(f\"[Student KD Conceptual] G Params: {gen_params_s:,}, D Params: {disc_params_s:,}, Total: {total_params_s:,}, Size: {model_size_mb_s:.2f}MB\")\n",
        "    except Exception as e_arch_stud: logger.error(f\"Student arch analysis error: {e_arch_stud}\", exc_info=True)\n",
        "\n",
        "    logger.warning(\"Skipping FLOPs for student generator.\")\n",
        "    architecture_stats_student['generator_gflops'] = 'Skipped'\n",
        "    stats_path_s = os.path.join(PROFILING_DIR, 'architecture_stats_student_kd_conceptual_a100.json') # Note conceptual\n",
        "    with open(stats_path_s, 'w') as f: json.dump(architecture_stats_student, f, indent=4)\n",
        "    logger.info(f\"Saved student arch stats to {stats_path_s}\")\n",
        "\n",
        "    # --- Profile Generation (Student) ---\n",
        "    logger.info(\"--- Profiling Conceptually Trained Student Model Generation ---\")\n",
        "    try:\n",
        "        input_size_s = gan_student_profiling.unwrapped_G.input_image_size\n",
        "        style_dim_s = gan_student_profiling.unwrapped_G.style_network.dim\n",
        "        gen_input_lowres_s = torch.randn(1, 3, input_size_s, input_size_s).to(current_device_stud)\n",
        "        gen_input_noise_s = torch.randn(1, style_dim_s).to(current_device_stud)\n",
        "        output_dir_student_imgs = os.path.join(PROFILING_DIR, \"generated_images_student_kd_conceptual\") # Note conceptual\n",
        "        os.makedirs(output_dir_student_imgs, exist_ok=True)\n",
        "\n",
        "        is_student_model_amp_fp16_prof = (gan_student_profiling.accelerator.state.mixed_precision == 'fp16')\n",
        "        if is_student_model_amp_fp16_prof:\n",
        "            gen_input_lowres_s, gen_input_noise_s = gen_input_lowres_s.half(), gen_input_noise_s.half()\n",
        "        else:\n",
        "            gen_input_lowres_s, gen_input_noise_s = gen_input_lowres_s.float(), gen_input_noise_s.float()\n",
        "\n",
        "        logger.info(f\"Warm-up for student profiling (Model AMP: {is_student_model_amp_fp16_prof}, Input: {gen_input_lowres_s.dtype})...\")\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast(device_type=current_device_stud.type, enabled=is_student_model_amp_fp16_prof):\n",
        "                _ = gan_student_profiling.generate(lowres_image=gen_input_lowres_s.clone(), noise=gen_input_noise_s.clone())\n",
        "        if current_device_stud.type == 'cuda': torch.cuda.synchronize()\n",
        "\n",
        "        logger.info(\"Starting student profiler...\")\n",
        "        activities_s = [ProfilerActivity.CPU]\n",
        "        if current_device_stud.type == 'cuda': activities_s.append(ProfilerActivity.CUDA)\n",
        "        with profile(activities=activities_s, record_shapes=True, profile_memory=True, with_stack=True) as prof_s:\n",
        "            with record_function(\"student_model_inference\"):\n",
        "                with torch.inference_mode():\n",
        "                    with torch.amp.autocast(device_type=current_device_stud.type, enabled=is_student_model_amp_fp16_prof):\n",
        "                        images_s = gan_student_profiling.generate(lowres_image=gen_input_lowres_s, noise=gen_input_noise_s)\n",
        "        if current_device_stud.type == 'cuda': torch.cuda.synchronize()\n",
        "        logger.info(\"Student profiling complete.\")\n",
        "        images_s = images_s.float().cpu().clamp(0., 1.)\n",
        "        save_image(images_s, os.path.join(output_dir_student_imgs, \"image_profiled_student_conceptual_a100.png\")) # Note conceptual\n",
        "        logger.info(f\"Saved profiled student image.\")\n",
        "        sort_key_s = \"cuda_time_total\" if current_device_stud.type == 'cuda' else \"cpu_time_total\"\n",
        "        print(prof_s.key_averages().table(sort_by=sort_key_s, row_limit=15))\n",
        "        profiler_output_path_s = os.path.join(PROFILING_DIR, 'profiler_student_conceptual_trace_a100.json') # Note conceptual\n",
        "        prof_s.export_chrome_trace(profiler_output_path_s)\n",
        "        logger.info(f\"Student profiler trace saved: {profiler_output_path_s}\")\n",
        "    except Exception as e_prof_stud: logger.error(f\"Student profiling error: {e_prof_stud}\", exc_info=True)\n",
        "    finally:\n",
        "        if 'images_s' in locals(): del images_s;\n",
        "        if 'gen_input_lowres_s' in locals(): del gen_input_lowres_s;\n",
        "        if 'gen_input_noise_s' in locals(): del gen_input_noise_s;\n",
        "        if 'prof_s' in locals(): del prof_s;\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache();\n",
        "        gc.collect()\n",
        "else:\n",
        "    logger.error(\"Trained student model not available for profiling.\")\n",
        "\n",
        "\n",
        "# Cell 12: Metrics Calculation (Trained Student Model)\n",
        "logger.info(\"--- GAN Evaluation Metrics Calculation (Conceptually Trained Student Model) ---\")\n",
        "# Re-define helper functions if this notebook is run standalone\n",
        "def calculate_metrics_from_directories(generated_dir, real_dir=None, batch_size=50, prefix=\"\"):\n",
        "    results = {};\n",
        "    if not os.path.isdir(generated_dir) or not glob.glob(os.path.join(generated_dir, \"*.[pj][np]g\")):\n",
        "        logger.error(f\"{prefix}Generated images directory is invalid or empty: {generated_dir}\"); return None\n",
        "    if real_dir and (not os.path.isdir(real_dir) or not glob.glob(os.path.join(real_dir, \"*.[pj][np]g\"))):\n",
        "        logger.warning(f\"{prefix}Real images directory is invalid or empty: {real_dir}. Skipping FID.\"); real_dir = None\n",
        "    try:\n",
        "        metrics_to_calc = {'isc': True, 'fid': bool(real_dir)}\n",
        "        logger.info(f\"{prefix}Calculating metrics: {metrics_to_calc} for {generated_dir}...\")\n",
        "        metrics_output = torch_fidelity.calculate_metrics(\n",
        "            input1=generated_dir, input2=real_dir, isc=metrics_to_calc['isc'], fid=metrics_to_calc['fid'],\n",
        "            batch_size=batch_size, cuda=torch.cuda.is_available(), save_cpu_ram=True, verbose=False\n",
        "        )\n",
        "        results['inception_score_mean'] = metrics_output.get('inception_score_mean', float('nan'))\n",
        "        results['inception_score_std'] = metrics_output.get('inception_score_std', float('nan'))\n",
        "        logger.info(f\"{prefix}Inception Score: {results['inception_score_mean']:.4f} ± {results['inception_score_std']:.4f}\")\n",
        "        if real_dir:\n",
        "            results['frechet_inception_distance'] = metrics_output.get('frechet_inception_distance', float('nan'))\n",
        "            logger.info(f\"{prefix}FID Score: {results['frechet_inception_distance']:.4f}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"{prefix}Error during metric calculation for {generated_dir}: {e}\", exc_info=True); results = None\n",
        "    return results\n",
        "\n",
        "def generate_images_for_metrics(gan_model, output_dir, num_images=100, batch_size=4, prefix=\"\"):\n",
        "    if gan_model is None: logger.error(f\"{prefix}GAN model is None.\"); return None\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    logger.info(f\"{prefix}Clearing existing images in {output_dir}...\")\n",
        "    for f in glob.glob(os.path.join(output_dir, \"*.*g\")): os.remove(f)\n",
        "\n",
        "    gan_model.eval(); num_batches = (num_images + batch_size - 1) // batch_size; image_count = 0\n",
        "    device = gan_model.accelerator.device\n",
        "    is_model_amp_fp16 = (gan_model.accelerator.state.mixed_precision == 'fp16')\n",
        "\n",
        "    unwrapped_G = getattr(gan_model, 'unwrapped_G', gan_model.G)\n",
        "    input_size = getattr(unwrapped_G, 'input_image_size', 64)\n",
        "    style_dim = getattr(getattr(unwrapped_G, 'style_network', None), 'dim', 512)\n",
        "\n",
        "    logger.info(f\"{prefix}Generating {num_images} images for metrics in {output_dir} (Model AMP: {is_model_amp_fp16})...\")\n",
        "    pbar = tqdm(range(num_batches), desc=f\"{prefix}Generating for {output_dir}\")\n",
        "    for i in pbar:\n",
        "        current_batch_size = min(batch_size, num_images - image_count)\n",
        "        if current_batch_size <= 0: break\n",
        "        lowres_images = torch.randn(current_batch_size, 3, input_size, input_size, device=device)\n",
        "        noise = torch.randn(current_batch_size, style_dim, device=device)\n",
        "\n",
        "        if is_model_amp_fp16: lowres_images, noise = lowres_images.half(), noise.half()\n",
        "        else: lowres_images, noise = lowres_images.float(), noise.float()\n",
        "\n",
        "        try:\n",
        "            with torch.inference_mode():\n",
        "                with torch.amp.autocast(device_type=device.type, enabled=is_model_amp_fp16):\n",
        "                    images_gen = gan_model.generate(lowres_image=lowres_images, noise=noise)\n",
        "            images_gen = images_gen.float().cpu().clamp(0., 1.)\n",
        "            for j, img_tensor in enumerate(images_gen):\n",
        "                if image_count >= num_images: break\n",
        "                save_image(img_tensor, os.path.join(output_dir, f'metric_image_{image_count:05d}.png'))\n",
        "                image_count += 1\n",
        "            pbar.set_postfix({\"images_generated\": image_count})\n",
        "        except Exception as e_gen: logger.error(f\"{prefix}Error gen batch {i}: {e_gen}\", exc_info=True); continue\n",
        "        finally:\n",
        "            del lowres_images, noise;\n",
        "            if 'images_gen' in locals(): del images_gen\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        if image_count >= num_images: break\n",
        "    logger.info(f\"{prefix}Generated {image_count} images to {output_dir}\")\n",
        "    return output_dir if image_count > 0 else None\n",
        "\n",
        "metrics_student_results = None\n",
        "# gan_student_profiling should be the loaded student model (conceptually trained in this case)\n",
        "if gan_student_profiling is not None:\n",
        "    metrics_output_dir_student = os.path.join(PROFILING_DIR, 'generated_images_for_metrics_student_kd_conceptual') # Note conceptual\n",
        "    gen_dir_stud = generate_images_for_metrics(gan_student_profiling, metrics_output_dir_student,\n",
        "                                               num_images=100, batch_size=8, prefix=\"[Student KD Conceptual Metrics] \") # Reduced num_images\n",
        "    if gen_dir_stud:\n",
        "        logger.info(\"Calculating metrics for CONCEPTUALLY TRAINED STUDENT (KD) model...\")\n",
        "        metrics_student_results = calculate_metrics_from_directories(\n",
        "            gen_dir_stud, real_dir=None, batch_size=16, prefix=\"[Student KD Conceptual Metrics] \") # real_dir=None for IS only\n",
        "else:\n",
        "    logger.error(\"Conceptually trained student model (gan_student_profiling) not available for metrics calculation.\")\n",
        "\n",
        "if metrics_student_results:\n",
        "    logger.info(f\"Final Student (KD Conceptual) Model Inception Score: {metrics_student_results.get('inception_score_mean', 'N/A'):.4f} ± {metrics_student_results.get('inception_score_std', 'N/A'):.4f}\")\n",
        "    if metrics_student_results.get('frechet_inception_distance', 'N/A') != 'N/A':\n",
        "        logger.info(f\"Final Student (KD Conceptual) Model FID Score: {metrics_student_results.get('frechet_inception_distance'):.4f}\")\n",
        "\n",
        "logger.info(\"Knowledge Distillation GigaGAN Notebook Finished\")\n",
        "if 'teacher_model' in locals(): del teacher_model\n",
        "if 'gan_student' in locals(): del gan_student\n",
        "if 'gan_student_profiling' in locals(): del gan_student_profiling\n",
        "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting validators\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.15.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=cf147baf37e4403fc0ad3c62a8586bcf1cfd96d00be52d074888d7ce11a3f576\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=888747231ed950db559d6b76953e91f7c149ba46fce89df41f5ce25bf490f7ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, xxhash, validators, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, iopath, nvidia-cusolver-cu12, fvcore, datasets, bitsandbytes, torch-fidelity\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 fvcore-0.1.5.post20221221 iopath-0.1.10 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 portalocker-3.1.1 torch-fidelity-0.3.0 validators-0.35.0 xxhash-3.5.0 yacs-0.1.8\n",
            "Collecting gigagan-pytorch==0.2.20\n",
            "  Downloading gigagan_pytorch-0.2.20-py3-none-any.whl.metadata (907 bytes)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (1.6.0)\n",
            "Collecting beartype (from gigagan-pytorch==0.2.20)\n",
            "  Downloading beartype-0.20.2-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: einops>=0.6 in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (0.8.1)\n",
            "Collecting ema-pytorch (from gigagan-pytorch==0.2.20)\n",
            "  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n",
            "Collecting kornia (from gigagan-pytorch==0.2.20)\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting numerize (from gigagan-pytorch==0.2.20)\n",
            "  Downloading numerize-0.12.tar.gz (2.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting open-clip-torch<3.0.0,>=2.0.0 (from gigagan-pytorch==0.2.20)\n",
            "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (11.2.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gigagan-pytorch==0.2.20) (4.67.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2024.11.6)\n",
            "Collecting ftfy (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (1.0.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->gigagan-pytorch==0.2.20) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->gigagan-pytorch==0.2.20) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate->gigagan-pytorch==0.2.20) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->gigagan-pytorch==0.2.20) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->gigagan-pytorch==0.2.20) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->gigagan-pytorch==0.2.20) (6.0.2)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia->gigagan-pytorch==0.2.20)\n",
            "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2.32.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->gigagan-pytorch==0.2.20) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->gigagan-pytorch==0.2.20) (2025.4.26)\n",
            "Downloading gigagan_pytorch-0.2.20-py3-none-any.whl (30 kB)\n",
            "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.20.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n",
            "Downloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: numerize\n",
            "  Building wheel for numerize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numerize: filename=numerize-0.12-py3-none-any.whl size=3153 sha256=a40121138601b9dc4cfecc8b3860ac36856afd8a76f891eec9a869b18b8d36c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/3e/5375462d832133c3684a27f9ad763a61141a802aee4bd445d6\n",
            "Successfully built numerize\n",
            "Installing collected packages: numerize, kornia_rs, ftfy, beartype, kornia, ema-pytorch, open-clip-torch, gigagan-pytorch\n",
            "Successfully installed beartype-0.20.2 ema-pytorch-0.7.7 ftfy-6.3.1 gigagan-pytorch-0.2.20 kornia-0.8.0 kornia_rs-0.1.9 numerize-0.12 open-clip-torch-2.32.0\n",
            "\n",
            "\n",
            "Generator: 43.71M\n",
            "Discriminator: 30.74M\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:NOTE: This loop uses PLACEHOLDER GAN losses and is conceptual.\n",
            "WARNING:__main__:Actual KD training requires implementing real GAN loss calculations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generator: 11.73M\n",
            "Discriminator: 8.06M\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:This was conceptual training. Load a properly trained student checkpoint before profiling.\n",
            "WARNING:__main__:Student checkpoint profiling_results_kd/student_model_final.ckpt not found. Profiling will be skipped or use in-memory model if available.\n",
            "WARNING:__main__:Skipping FLOPs for student generator.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                student_model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     101.487ms       361.26%     101.487ms     101.487ms           0 b           0 b           0 b           0 b             1  \n",
            "                                student_model_inference        38.12%      42.546ms        91.09%     101.664ms     101.664ms       0.000us         0.00%      28.092ms      28.092ms           0 b           0 b     768.00 Kb      -2.01 Gb             1  \n",
            "                                           aten::einsum         0.67%     749.252us         2.43%       2.714ms     123.377us       0.000us         0.00%      19.862ms     902.838us           0 b           0 b     266.00 Mb           0 b            22  \n",
            "                                              aten::bmm         0.74%     830.840us         1.01%       1.132ms      51.439us      19.862ms        70.70%      19.862ms     902.838us           0 b           0 b     266.00 Mb     266.00 Mb            22  \n",
            "                                ampere_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      18.834ms        67.04%      18.834ms       1.712ms           0 b           0 b           0 b           0 b            11  \n",
            "                                           aten::conv2d         0.59%     655.330us        12.65%      14.117ms     114.773us       0.000us         0.00%       2.313ms      18.804us           0 b           0 b     699.31 Mb           0 b           123  \n",
            "                                      aten::convolution         0.65%     723.847us        12.06%      13.462ms     109.446us       0.000us         0.00%       2.313ms      18.804us           0 b           0 b     699.31 Mb           0 b           123  \n",
            "                                     aten::_convolution         1.51%       1.690ms        11.41%      12.738ms     103.561us       0.000us         0.00%       2.313ms      18.804us           0 b           0 b     699.31 Mb           0 b           123  \n",
            "                                aten::cudnn_convolution         5.39%       6.019ms         7.90%       8.812ms      74.682us       2.029ms         7.22%       2.029ms      17.195us           0 b           0 b     698.31 Mb     698.31 Mb           118  \n",
            "                                          aten::softmax         0.20%     225.967us         2.63%       2.936ms      46.608us       0.000us         0.00%       1.756ms      27.871us           0 b           0 b     452.15 Mb           0 b            63  \n",
            "                                         aten::_softmax         0.97%       1.084ms         2.43%       2.710ms      43.022us       1.756ms         6.25%       1.756ms      27.871us           0 b           0 b     452.15 Mb     452.15 Mb            63  \n",
            "                                       aten::group_norm         0.29%     322.024us         4.97%       5.544ms     120.521us       0.000us         0.00%       1.171ms      25.457us           0 b           0 b      37.25 Mb     -46.00 Kb            46  \n",
            "                                aten::native_group_norm         2.09%       2.329ms         4.68%       5.222ms     113.521us       1.171ms         4.17%       1.171ms      25.457us           0 b           0 b      37.29 Mb     -50.00 Kb            46  \n",
            "                                              aten::mul         3.10%       3.465ms         4.76%       5.312ms      23.504us       1.130ms         4.02%       1.130ms       4.998us           0 b           0 b     381.20 Mb     381.20 Mb           226  \n",
            "void at::native::(anonymous namespace)::cunn_Spatial...         0.00%       0.000us         0.00%       0.000us       0.000us       1.029ms         3.66%       1.029ms     171.504us           0 b           0 b           0 b           0 b             6  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 111.612ms\n",
            "Self CUDA time total: 28.092ms\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Student KD Conceptual Metrics] Generating for profiling_results_kd/generated_images_for_metrics_student_kd_conceptual:  92%|█████████▏| 12/13 [00:04<00:00,  2.72it/s, images_generated=100]\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 263MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84729"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyNk-VfaY2Ae",
        "outputId": "9dce0f09-9df6-4e4b-a0fa-de8fa36244c2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P0yEYAeUddGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}